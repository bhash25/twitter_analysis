# -*- coding: utf-8 -*-
"""Twitter Trend Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H62iHcNJgE40dHJU7ai6ExeLZchyX-pQ
"""

from tweepy import OAuthHandler
from tweepy import API
import tweepy
import json

# Consumer key authentication(consumer_key,consumer_secret can be collected from our twitter developer profile)
auth = OAuthHandler('kqV2Z6jN4VZ1Fkvc2rcwJz99O', 'WTqH4ni1A6R3AC9KZMHsTAQSsRuAjQBfsqwAYSsVsLnNcZ2eLw')

# Access key authentication(access_token,access_token_secret can be collected from our twitter developer profile)
auth.set_access_token('1400343844479066123-8jajVJg3gqoFzl0ZregpvE9vAO7zXw', 'ecwCLXNb4t9POoK4YTQmY6eDQhX4EQNmVpoedJ40vLP5M')

# Set up the API with the authentication handler
api = API(auth)



from tweepy import Stream
import time

# Set up words to track
keywords_to_track = ['#anipoke','#pokemon']




# Streaming Tweets
#override tweepy.StreamListener to add logic to on_status
class MyStreamListener(tweepy.StreamListener):

    def __init__(self):
        super(tweepy.StreamListener, self).__init__()
        self.save_file = open('tweets.json','w')
        self.tweets = []

    def on_status(self, status):
        print(status.text)

    def on_data(self, tweet):
        self.tweets.append(json.loads(tweet))
        self.save_file.write(str(tweet))

    def on_error(self, status):
        print(status)
        return True

# Initialize Stream listener
listen = MyStreamListener()

# Creating Stream object with authentication
# stream = tweepy.Stream(auth, l)

# Filter Twitter Streams to capture data by the keywords:
# stream.filter(track = ['clinton', 'trump', 'sanders', 'cruz'])


# Instantiate the Stream object
stream = tweepy.Stream(auth, listen)

# Begin collecting data
stream.filter(track = keywords_to_track, is_async=True)

runtime = 60 # this means for that many seconds the Streaming API connection will be established.


# twitterstream = Stream(auth, StreamListener())

# twitterstream.filter(track=['twitter'], async=True) #apply any filter you want

time.sleep(runtime) #halts the control for runtime seconds

stream.disconnect() #disconnect the stream and stop streaming

tweets = []
import glob
files  = list(glob.iglob('/content/tweets (2).json'))
for f in files:
    fh = open(f, 'r', encoding = 'utf-8')
    tweets_json = fh.read().split("\n")

    ## remove empty lines
    tweets_json = list(filter(len, tweets_json))

    ## parse each tweet
    for tweet in tweets_json:
        tweet_obj = json.loads(tweet)

        # Store the user screen name in 'user-screen_name'
        tweet_obj['user-screen_name'] = tweet_obj['user']['screen_name']

        # Check if this is a 140+ character tweet
        if 'extended_tweet' in tweet_obj:
            # Store the extended tweet text in 'extended_tweet-full_text'
            tweet_obj['extended_tweet-full_text'] = tweet_obj['extended_tweet']['full_text']

        if 'retweeted_status' in tweet_obj:
            # Store the retweet user screen name in 'retweeted_status-user-screen_name'
            tweet_obj['retweeted_status-user-screen_name'] = tweet_obj['retweeted_status']['user']['screen_name']

            # Store the retweet text in 'retweeted_status-text'
            tweet_obj['retweeted_status-text'] = tweet_obj['retweeted_status']['text']

        if 'quoted_status' in tweet_obj:
            # Store the retweet user screen name in 'retweeted_status-user-screen_name'
            tweet_obj['quoted_status-user-screen_name'] = tweet_obj['quoted_status']['user']['screen_name']

            # Store the retweet text in 'retweeted_status-text'
            tweet_obj['quoted_status-text'] = tweet_obj['quoted_status']['text']

        tweets.append(tweet_obj)

tweets

import pandas as pd

df_tweet = pd.DataFrame(tweets)
df_tweet

print(df_tweet.columns)

"""Checks if a word is in a Twitter dataset's text.
    Checks text and extended tweet (140+ character tweets) for tweets,
    retweets and quoted tweets.
    Will Return a logical pandas Series.
    """


    """def check_word_in_tweet(word, df_tweet):


      contains_column = df_tweet['text'].str.contains(word, case = False)
      contains_column |= df_tweet['extended_tweet-full_text'].str.contains(word, case = False)
      contains_column |= df_tweet['quoted_status-text'].str.contains(word, case = False)
      contains_column |= df_tweet['retweeted_status-text'].str.contains(word, case = False)

      return contains_column """

"""import pandas as pd
def check_word_in_tweet(word,data,column):
  contains_column = data[column].str.contains('Anipoke',case=False)
  return contains_columns
contains_column = check_word_in_tweet('Anipoke',df_tweet,'text') | check_word_in_tweet('Anipoke',df_tweet,'extended_tweet-full_text') | check_word_in_tweet('anipoke',df_tweet,'quoted_status-text') | check_word_in_tweet('anipoke',df_tweet,'retweeted_status-text')
 """
"""Checks if a word is in a Twitter dataset's text.
    Checks text and extended tweet (140+ character tweets) for tweets,
    retweets and quoted tweets.
    Returns a logical pandas Series."""

def check_word_in_tweet(word, data):
     contains_column = data['text'].str.contains(word, case = False)
     contains_column |= data['extended_tweet-full_text'].str.contains(word, case = False)
     contains_column |= data['quoted_status-text'].str.contains(word, case = False)
     contains_column |= data['retweeted_status-text'].str.contains(word, case = False)
     return contains_column



import numpy as np
# Finding mentions of #anipoke in all text fields
anipoke = check_word_in_tweet('anipoke', df_tweet)
print(anipoke)
# Finding mentions of #pokemon in all text fields
pokemon = check_word_in_tweet('pokemon', df_tweet)
print(pokemon)
# Print proportion of tweets mentioning #anipoke
print("Proportion of #anipoke tweets:", np.sum(anipoke) / df_tweet.shape[0])

# Print proportion of tweets mentioning #pokemon
print("Proportion of #pokemon tweets:", np.sum(pokemon) / df_tweet.shape[0])

# Print created_at to see the original format of datetime in Twitter data
print(df_tweet['created_at'].head())

# Convert the created_at column to np.datetime object
df_tweet['created_at'] = pd.to_datetime(df_tweet['created_at'])

# Print created_at to see new format
print(df_tweet['created_at'].head())

# Set the index of df_tweet to created_at
df_tweet = df_tweet.set_index('created_at')

# Create a column for Anipoke
df_tweet['anipoke'] = check_word_in_tweet('anipoke', df_tweet)

# Create a column for Pokemon
df_tweet['pokemon'] = check_word_in_tweet('pokemon', df_tweet)

import matplotlib.pyplot as plt
from matplotlib import style
style.use('fivethirtyeight')
# Average of Anipoke column by day
mean_anpk = df_tweet['anipoke'].resample('1 min').mean()

# (This function will group the data minute wise and apply the mean function to the grouped data.)

# Average of Pokemon column by day
mean_pkmn = df_tweet['pokemon'].resample('1 min').mean()

# Plot mean Anipoke/Pokemon by day
plt.plot(mean_anpk.index.minute, mean_anpk)
plt.plot(mean_pkmn.index.minute, mean_pkmn)

# Add labels and show
plt.xlabel('Minute'); plt.ylabel('Frequency')
plt.title('Trend mentions over time')
plt.legend(('#anipoke', '#pokemon'))
plt.show()

df_tweet=df_tweet[['text','source','user','lang','anipoke','pokemon']]

from bs4 import BeautifulSoup
from nltk.tokenize import WordPunctTokenizer
token = WordPunctTokenizer()

def cleaning_tweets(t):
    del_amp = BeautifulSoup(t, 'lxml')
    del_amp_text = del_amp.get_text()
    del_link_mentions = re.sub(combined_re, '', del_amp_text)
    del_emoticons = re.sub(regex_pattern, '', del_link_mentions)
    lower_case = del_emoticons.lower()
    words = token.tokenize(lower_case)
    result_words = [x for x in words if len(x) > 2]
    return (" ".join(result_words)).strip()

df_tweet.shape

print(df_tweet['text'].head())

#Extract textfields from tweets
raw_tweets = list(df_tweet['text'])
raw_tweets

!pip install twython
# Import the Twython class
from twython import Twython

import numpy as np
import matplotlib.pyplot as plt
import re
from PIL import Image
from wordcloud import WordCloud, STOPWORDS
from IPython.display import Image as im

import re

#Create a string form of our list of text
raw_string = ''.join(raw_tweets)
no_links = re.sub(r'http\S+', '', raw_string)
no_unicode = re.sub(r"\\[a-z][a-z]?[0-9]+", '', no_links)
no_special_characters = re.sub('[^A-Za-z ]+', '', no_unicode)

words = no_special_characters.split(" ")
words = [w for w in words if len(w) > 2]  # ignore a, an, be, ...
words = [w.lower() for w in words]
words = [w for w in words if w not in STOPWORDS]

mask = np.array(Image.open('/content/il_fullxfull.982360484_sngr.webp'))

wc = WordCloud(background_color="white", max_words=2000, mask=mask)
clean_string = ','.join(words)
wc.generate(clean_string)

f = plt.figure(figsize=(50,50))
f.add_subplot(1,2, 1)
plt.imshow(mask, cmap=plt.cm.gray, interpolation='bilinear')
plt.title('Original Stencil', size=40)
plt.axis("off")
f.add_subplot(1,2, 2)
plt.imshow(wc, interpolation='bilinear')
plt.title('Twitter Generated Cloud', size=40)
plt.axis("off")
plt.show()

# Print the columns of the dataframe
print(df_tweet.columns)

import seaborn as sns
import matplotlib.pyplot as plt

# Set seaborn style
sns.set(color_codes=True)

# Create a list of labels:cd
cd = ['anipoke', 'pokemon']

anpk = df_tweet['anipoke'].sum()
pkmn= df_tweet['pokemon'].sum()

# Plot histogram
ax = sns.barplot(cd, [anpk, pkmn])
ax.set(ylabel="count")
plt.show()

import random
import numpy
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from textblob import TextBlob
from matplotlib import pyplot as plt

df_tweet['source']=df_tweet['source'].apply(lambda x: x[x.find('>')+1:])
df_tweet['source']=df_tweet['source'].apply(lambda x: x[:x.find('<')])

df_tweet['source']

anipoke=df_tweet[df_tweet['anipoke']==True]
pokemon=df_tweet[df_tweet['pokemon']==True]

from pandas import json_normalize

anipoke_user_info=json_normalize(anipoke['user'])
pokemon_user_info=json_normalize(pokemon['user'])

anipoke_user_info=anipoke_user_info[['location','followers_count','friends_count','statuses_count','favourites_count']]
pokemon_user_info=anipoke_user_info[['location','followers_count','friends_count','statuses_count','favourites_count']]

df_tweet.columns

sns.countplot(df_tweet['lang'])

sns.countplot(anipoke['lang'])

sns.countplot(pokemon['lang'])

tweet_df_5min = df_tweet.groupby(pd.Grouper(key='created_at', freq='5 min', convention='start')).size()
tweet_df_5min.plot(figsize=(18,6))
plt.ylabel('5 Minute Tweet Count')
plt.title('#anipoke and #pokemon tweets')
plt.grid(True)

from matplotlib_venn import venn2

campaign_a = df_tweet[(df_tweet['anipoke'] == 1)]
campaign_b = df_tweet[(df_tweet['pokemon'] == 1)]

plt.figure(figsize=(4,4))
set1 = set(campaign_a['id'])
set2 = set(campaign_b['id'])

venn2([set1, set2], ('Set1', 'Set2'))
plt.show()